{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL(Glove).ipynb","version":"0.3.2","provenance":[{"file_id":"1wumH6IW34rJDVfIujKK8I5DpCCkxSUgP","timestamp":1556127299705}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"OuKO4r9Z5IEF","colab_type":"code","outputId":"4f0b05ed-4658-4914-a0ed-2bbc0843503b","executionInfo":{"status":"ok","timestamp":1556157982502,"user_tz":300,"elapsed":436,"user":{"displayName":"Jie He","photoUrl":"","userId":"02992744115617994185"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":80,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"YZuC4slv5LCd","colab_type":"code","outputId":"456482d8-2a62-4229-e205-7c21cbdadab9","executionInfo":{"status":"ok","timestamp":1556157986879,"user_tz":300,"elapsed":2654,"user":{"displayName":"Jie He","photoUrl":"","userId":"02992744115617994185"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"cell_type":"code","source":["!ls '/content/drive/Team Drives/ANLY-521 Final'"],"execution_count":81,"outputs":[{"output_type":"stream","text":[" business_reviews2017.tsv   FinalProjectDescription.pdf\t\t    model\n"," data\t\t\t    Glove_Global_Vectors\t\t    script\n","'DL(Glove).ipynb'\t   'Literature Review - Brainstorm .gdoc'\n","'Final Deliverable'\t   'Literature Review.gdoc'\n"],"name":"stdout"}]},{"metadata":{"id":"FSlPreGJ6whZ","colab_type":"code","outputId":"e9c4dea2-d65b-429d-be6d-22ae7403f74d","executionInfo":{"status":"ok","timestamp":1556157991458,"user_tz":300,"elapsed":3111,"user":{"displayName":"Jie He","photoUrl":"","userId":"02992744115617994185"}},"colab":{"base_uri":"https://localhost:8080/","height":175}},"cell_type":"code","source":["pip install paramiko"],"execution_count":82,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: paramiko in /usr/local/lib/python3.6/dist-packages (2.4.2)\n","Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from paramiko) (1.3.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from paramiko) (0.4.5)\n","Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.6/dist-packages (from paramiko) (3.1.6)\n","Requirement already satisfied: cryptography>=1.5 in /usr/local/lib/python3.6/dist-packages (from paramiko) (2.6.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pynacl>=1.0.1->paramiko) (1.12.0)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from pynacl>=1.0.1->paramiko) (1.12.3)\n","Requirement already satisfied: asn1crypto>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from cryptography>=1.5->paramiko) (0.24.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.4.1->pynacl>=1.0.1->paramiko) (2.19)\n"],"name":"stdout"}]},{"metadata":{"id":"FiNQll2xHrQS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1005},"outputId":"7352c79d-28b6-420d-e87a-ed43ef093442","executionInfo":{"status":"ok","timestamp":1556158927308,"user_tz":300,"elapsed":931645,"user":{"displayName":"Jie He","photoUrl":"","userId":"02992744115617994185"}}},"cell_type":"code","source":["import argparse\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from gensim.models import KeyedVectors\n","import re\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.preprocessing import text\n","from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Conv1D, MaxPooling1D, Bidirectional\n","from sklearn.metrics import accuracy_score, f1_score\n","# from keras.models import load_model\n","\n","\n","class LoadData:\n","    '''\n","    Load, classify and split data\n","    '''\n","    def __init__(self, data_file, out_path, verbose=True):\n","        self.data = pd.read_csv(data_file, sep = '\\t', index_col = 0)\n","        # assign review samples to two classes using [0,4) and [4, 5] criteria\n","        self.data['class'] = (self.data['stars'] >= 4).astype(int)\n","        self.data = self.data[['text', 'class']]\n","        self.data['text'] = self.data['text'].apply(CleanText)\n","        np.random.seed(1)\n","        self.train, self.test = train_test_split(self.data, train_size=0.7)\n","        # optional file saving\n","        if verbose:\n","            self.data.to_csv(out_path + '.tsv', sep='\\t', index=False)\n","            self.train.to_csv(out_path+'_train.tsv', sep='\\t', index=False)\n","            self.test.to_csv(out_path+'_test.tsv', sep='\\t', index=False)\n","\n","\n","def CleanText(string):\n","    '''\n","    String cleaning\n","    :param string:\n","    :return: Cleaned review text\n","    '''\n","    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n","    string = re.sub(r\" \\'s\", \"\\'s\", string)\n","    string = re.sub(r\" \\'ve\", \"\\'ve\", string)\n","    string = re.sub(r\" n\\'t\", \"n\\'t\", string)\n","    string = re.sub(r\" \\'re\", \"\\'re\", string)\n","    string = re.sub(r\" \\'d\", \"\\'d\", string)\n","    string = re.sub(r\" \\'ll\", \"\\'ll\", string)\n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\" \\(\", \"\", string)\n","    string = re.sub(r\" \\)\", \"\", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\n\", \" \", string)\n","    return string\n","\n","\n","def Padding(data, max_len = 50):\n","    '''\n","    Padding vector to specified length\n","    :param data: Text input\n","    :param max_len: Padding length\n","    :return: Padded vector\n","    '''\n","    return pad_sequences(data, padding='post', truncating='post', maxlen = max_len)\n","\n","\n","def WordEmbedding(X, y, embed_path='model/GoogleNews-vectors-negative300.bin',\n","                  max_features = 3000, w2v_size = 300, max_len = 50):\n","    '''\n","    Create word embedding\n","    :param X: train data input\n","    :param y: train data label\n","    :param embed_path: Path to pre-trained word2vec model\n","    :param max_features: Maximum number of features\n","    :param w2v_size: Word2vec size\n","    :param max_len: text padding length\n","    :return: Processed training data input and label, tokenizer, word embedding\n","    '''\n","    # tokenization & vectorizization\n","    tk = text.Tokenizer(num_words=max_features, filters='\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n',\n","                        split=\" \")\n","    tk.fit_on_texts(X)\n","    word_index = tk.word_index\n","    # padding\n","    X_train = Padding(tk.texts_to_sequences(X), max_len)\n","    y_train = y\n","    # load google news pre-trained model\n","    w2v_model = KeyedVectors.load_word2vec_format(embed_path, binary=True)\n","    # Create word embedding vector matrix using pre-trained model\n","    w2v_matrix = np.zeros((len(word_index) + 1, w2v_size))\n","    for word,i in word_index.items():\n","        if word in w2v_model.vocab:\n","            w2v_matrix[i] = w2v_model[word]\n","    w2v_emb = Embedding(len(word_index)+1, w2v_size, weights=[w2v_matrix],\n","                            input_length=max_len)\n","    return X_train, y_train, tk, w2v_emb\n","\n","\n","def WordEmbedding_1(X, y, max_features = 3000, w2v_size = 300, max_len = 50):\n","    '''\n","    This one is for GLOVE Embedding from Stanford\n","    '''\n","    # tokenization\n","    tk = text.Tokenizer(num_words=max_features, filters='\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n', \n","                        split=\" \")\n","    tk.fit_on_texts(X)\n","    word_index = tk.word_index\n","    # padding\n","    X_train = Padding(tk.texts_to_sequences(X), max_len)\n","    y_train = y                \n","    \n","    # prepare the embedding layer by using GLOVE\n","    GLOVE_dir = '/content/drive/Team Drives/ANLY-521 Final/Glove_Global_Vectors'\n","\n","    embedding_ind = {}\n","    file = open(os.path.join(GLOVE_dir, 'glove.6B.50d.txt'))\n","    for line in file:      \n","      values = line.split()\n","      word = values[0]\n","      coefs = np.asarray(values[1:], dtype='float32')\n","      embedding_ind[word] = coefs\n","    file.close()\n","\n","    # compute embedding matrix\n","    embedding_dim = 50 # size of each word vector\n","\n","    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n","    for word, i in word_index.items():\n","        embedding_vector = embedding_ind.get(word)\n","        if embedding_vector is not None:\n","          # words not found treated as all zeros\n","          embedding_matrix[i] = embedding_vector\n","    \n","    # define embedding layer\n","    embedding_layer = Embedding(len(word_index) + 1, \n","                                embedding_dim,\n","                                weights=[embedding_matrix],\n","                                input_length=max_len,\n","                                trainable=False)\n","    \n","    return X_train, y_train, tk, embedding_layer  \n","  \n","  \n","def base_LSTM(X, y, w2v_emb, output_size=100, dropout=0.2,\n","              loss='binary_crossentropy', optimizer='adam',\n","              batch_size=128, nb_epoch=10, validation_split=0.2,\n","              shuffle=True):\n","    '''\n","    LSTM Model\n","    :param X: Train data input\n","    :param y: Train data label\n","    :param w2v_emb: Word embedding\n","    :param output_size: Output size\n","    :param dropout: Dropout ratio\n","    :param loss: Loss function\n","    :param optimizer: Optimizer\n","    :param batch_size: Batch size\n","    :param nb_epoch: Number of epoch\n","    :param validation_split: Training / Validation split\n","    :param shuffle: Shuffle training data before each epoch\n","    :return: LSTM model\n","    '''\n","    model = Sequential()\n","    model.add(w2v_emb)\n","    model.add(LSTM(output_size))\n","    model.add(Dropout(dropout))\n","    model.add(Dense(1))\n","    model.add(Activation('sigmoid'))\n","    model.layers[1].trainable = False\n","    model.compile(loss=loss,\n","                  optimizer=optimizer,\n","                  metrics=['accuracy'])\n","    model.fit(X, y, batch_size=batch_size,\n","              epochs=nb_epoch, validation_split=validation_split,\n","              shuffle=shuffle)\n","\n","    return model\n","\n","\n","def CNNLSTM(X, y, w2v_emb, output_size=100, dropout=0.2,\n","            loss='binary_crossentropy', optimizer='adam',\n","            batch_size=128, nb_epoch=10, validation_split=0.2,\n","            shuffle=True):\n","    '''\n","    CNN + LSTM Model\n","    :param X: Train data input\n","    :param y: Train data label\n","    :param w2v_emb: Word embedding\n","    :param output_size: Output size\n","    :param dropout: Dropout ratio\n","    :param loss: Loss function\n","    :param optimizer: Optimizer\n","    :param batch_size: Batch size\n","    :param nb_epoch: Number of epoch\n","    :param validation_split: Training / Validation split\n","    :param shuffle: Shuffle training data before each epoch\n","    :return: CNN + LSTM model\n","    '''\n","    model = Sequential()\n","    model.add(w2v_emb)\n","    model.add(Conv1D(64, 5, activation='relu'))\n","    model.add(Dropout(dropout))\n","    model.add(MaxPooling1D(pool_size=4))\n","    model.add(LSTM(output_size))\n","    model.add(Dense(1))\n","    model.add(Activation('sigmoid'))\n","    model.layers[1].trainable = False\n","    model.compile(loss=loss,\n","                  optimizer=optimizer,\n","                  metrics=['accuracy'])\n","    model.fit(X, y, batch_size=batch_size,\n","              epochs=nb_epoch, validation_split=validation_split,\n","              shuffle=shuffle)\n","\n","    return model\n","\n","\n","def BiLSTM(X, y, w2v_emb, output_size=100, dropout=0.2,\n","           loss='binary_crossentropy', optimizer='adam',\n","           batch_size=128, nb_epoch=10, validation_split=0.2,\n","           shuffle=True):\n","    '''\n","    CNN + Bidirectional LSTM Model\n","    :param X: Train data input\n","    :param y: Train data label\n","    :param w2v_emb: Word embedding\n","    :param output_size: Output size\n","    :param dropout: Dropout ratio\n","    :param loss: Loss function\n","    :param optimizer: Optimizer\n","    :param batch_size: Batch size\n","    :param nb_epoch: Number of epoch\n","    :param validation_split: Training / Validation split\n","    :param shuffle: Shuffle training data before each epoch\n","    :return: CNN + Bidirectional model\n","    '''\n","    model = Sequential()\n","    model.add(w2v_emb)\n","    model.add(Conv1D(64, 5, activation='relu'))\n","    model.add(Dropout(dropout))\n","    model.add(MaxPooling1D(pool_size=4))\n","    model.add(Bidirectional(LSTM(output_size)))\n","    model.add(Dense(1))\n","    model.add(Activation('sigmoid'))\n","    model.layers[1].trainable = False\n","    model.compile(loss=loss,\n","                  optimizer=optimizer,\n","                  metrics=['accuracy'])\n","    model.fit(X, y, batch_size=batch_size,\n","              epochs=nb_epoch, validation_split=validation_split,\n","              shuffle=shuffle)\n","\n","    return model\n","\n","\n","def BiLSTM_1(X, y, embedding_layer, max_len = 100, output_size = 50, dropout = 0.1, \n","            loss = 'binary_crossentropy', optimizer = 'adam', \n","            batch_size = 128, nb_epoch = 10, validation_split = 0.2, \n","            shuffle = True):\n","    '''\n","    This one is for using GLOVE embedding from Stanford\n","    '''  \n","    inp = Input(shape=(max_len,))\n","    x = embedded_sequences = embedding_layer(inp)\n","    x = Bidirectional(LSTM(output_size, return_sequences=True, dropout=dropout, recurrent_dropout=0.1))(x)\n","    x = GlobalMaxPool1D()(x)\n","    x = Dense(50, activation=\"relu\")(x)\n","    x = Dropout(0.1)(x)\n","    x = Dense(1, activation=\"sigmoid\")(x)\n","    model = Model(inputs=inp, outputs=x)\n","    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n","   \n","    # fit model\n","    model.fit(X, y, batch_size=batch_size, epochs=nb_epoch, \n","              validation_split=validation_split, \n","              shuffle=shuffle)\n","  \n","    return model\n","      \n","  \n","def Eval(X, y, name, model, verbose = True,\n","         output_path='model/'):\n","    '''\n","    Model evaluation\n","    :param X: Test data input\n","    :param y: Test data label\n","    :param model: Model\n","    :return: Accuracy and F1 scores\n","    '''\n","    print(f'{name} Model')\n","    X_test = X\n","    y_test = y\n","    y_pred = model.predict_classes(X_test)\n","    print(f'Test Accuracy:{accuracy_score(y_test, y_pred)}')\n","    print(f'Test F1:{f1_score(y_test, y_pred)}')\n","    if verbose:\n","        model.save(f'{output_path}{name}.h5')\n","        \n","\n","def Eval_1(X, y, name, model, verbose = True,\n","         output_path='model/'):\n","    '''\n","    Model evaluation for non-sequential\n","    :param X: Test data input\n","    :param y: Test data label\n","    :param model: Model\n","    :return: Accuracy and F1 scores\n","    '''\n","    print(f'{name} Model')\n","    X_test = X\n","    y_test = y\n","    test_acc = model.evaluate(X_test, y_test, verbose=0)\n","    print(f'Test Accuracy:{test_acc}')\n","    if verbose:\n","        model.save(f'{output_path}{name}.h5')\n","  \n","\n","def main():\n","\n","    # Load, classify and split data\n","    data_file = '/content/drive/Team Drives/ANLY-521 Final/data/business_reviews2017.tsv'\n","    out_path = '/content/drive/Team Drives/ANLY-521 Final/data/business_reviews'    \n","    DF = LoadData(data_file, out_path, verbose=False)\n","\n","    # visualize the distribution of each class\n","    #ax = DF.data['class'].value_counts().plot(kind='bar',figsize=(14,8),\n","    #            title=\"Number for Each Class (1 = high star, 0 = low star)\")\n","    #ax.set_xlabel(\"Class\")\n","    #ax.set_ylabel(\"Count\")\n","    #plt.show()\n","\n","    # data pre-processing\n","    #X_train, y_train, tk, w2v_emb = WordEmbedding(DF.train['text'], DF.train['class'], max_len = 100)\n","    X_train_1, y_train_1, tk_1, embedding_layer = WordEmbedding_1(DF.train['text'], DF.train['class'], max_len = 100) \n","\n","    # model training\n","    LSTM_model = base_LSTM(X_train_1, y_train_1, embedding_layer, output_size = 64)\n","    CNNLSTM_model = CNNLSTM(X_train_1, y_train_1, embedding_layer, output_size = 64, nb_epoch = 3)\n","    biLSTM_model = BiLSTM(X_train_1, y_train_1, embedding_layer, output_size = 64, nb_epoch = 3)\n","    biLSTM_model_1 = BiLSTM_1(X_train_1, y_train_1, embedding_layer, max_len = 100, output_size = 64, nb_epoch = 3)\n","    \n","    # model performance on test data\n","    X_test = Padding(tk.texts_to_sequences(DF.test['text']), max_len = 100)\n","    Eval(X_test, DF.test['class'], 'LSTM', LSTM_model, verbose=False)\n","    Eval(X_test, DF.test['class'], 'CNN + LSTM', CNNLSTM_model, verbose=False)\n","    Eval(X_test, DF.test['class'], 'CNN + Bidirectional LSTM', biLSTM_model, verbose=False)\n","    Eval_1(X_test, DF.test['class'], 'CNN + Bidirectional LSTM with GLOVE', biLSTM_model_1, verbose=False)\n","    \n"," \n","\n"," #if __name__ == \"__main__\":\n","\n","#    parser = argparse.ArgumentParser()\n","#    parser.add_argument(\"--data_file\", type=str,\n","#                        default=\"data/business_reviews2017.tsv\",\n","#                        help=\"2017 Yelp Business Reviews tsv file\")\n","#    parser.add_argument(\"--out_path\", type=str,\n","#                        default=\"data/business_reviews\",\n","#                        help=\"Dir to write train/test data\")\n","\n","#    args = parser.parse_args()\n","\n","#    main(args.data_file, args.out_path)\n","main()"],"execution_count":83,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 41341 samples, validate on 10336 samples\n","Epoch 1/10\n","41341/41341 [==============================] - 28s 685us/step - loss: 0.6933 - acc: 0.5271 - val_loss: 0.6878 - val_acc: 0.5498\n","Epoch 2/10\n","41341/41341 [==============================] - 24s 571us/step - loss: 0.6886 - acc: 0.5448 - val_loss: 0.6855 - val_acc: 0.5527\n","Epoch 3/10\n","41341/41341 [==============================] - 23s 563us/step - loss: 0.6872 - acc: 0.5471 - val_loss: 0.6847 - val_acc: 0.5584\n","Epoch 4/10\n","41341/41341 [==============================] - 23s 549us/step - loss: 0.6861 - acc: 0.5488 - val_loss: 0.6838 - val_acc: 0.5616\n","Epoch 5/10\n","41341/41341 [==============================] - 23s 549us/step - loss: 0.6856 - acc: 0.5513 - val_loss: 0.6832 - val_acc: 0.5627\n","Epoch 6/10\n","41341/41341 [==============================] - 24s 588us/step - loss: 0.6844 - acc: 0.5547 - val_loss: 0.6827 - val_acc: 0.5647\n","Epoch 7/10\n","41341/41341 [==============================] - 25s 606us/step - loss: 0.6843 - acc: 0.5565 - val_loss: 0.6827 - val_acc: 0.5656\n","Epoch 8/10\n","41341/41341 [==============================] - 23s 546us/step - loss: 0.6845 - acc: 0.5537 - val_loss: 0.6820 - val_acc: 0.5654\n","Epoch 9/10\n","41341/41341 [==============================] - 24s 576us/step - loss: 0.6836 - acc: 0.5578 - val_loss: 0.6817 - val_acc: 0.5665\n","Epoch 10/10\n","41341/41341 [==============================] - 22s 544us/step - loss: 0.6843 - acc: 0.5562 - val_loss: 0.6822 - val_acc: 0.5653\n","Train on 41341 samples, validate on 10336 samples\n","Epoch 1/3\n","41341/41341 [==============================] - 18s 440us/step - loss: 0.6712 - acc: 0.5801 - val_loss: 0.6482 - val_acc: 0.6176\n","Epoch 2/3\n","41341/41341 [==============================] - 12s 302us/step - loss: 0.6427 - acc: 0.6299 - val_loss: 0.6296 - val_acc: 0.6442\n","Epoch 3/3\n","41341/41341 [==============================] - 13s 302us/step - loss: 0.6349 - acc: 0.6388 - val_loss: 0.6242 - val_acc: 0.6484\n","Train on 41341 samples, validate on 10336 samples\n","Epoch 1/3\n","41341/41341 [==============================] - 33s 809us/step - loss: 0.6579 - acc: 0.6046 - val_loss: 0.6504 - val_acc: 0.6147\n","Epoch 2/3\n","41341/41341 [==============================] - 26s 632us/step - loss: 0.6404 - acc: 0.6326 - val_loss: 0.6300 - val_acc: 0.6453\n","Epoch 3/3\n","41341/41341 [==============================] - 27s 642us/step - loss: 0.6320 - acc: 0.6398 - val_loss: 0.6350 - val_acc: 0.6368\n","Train on 41341 samples, validate on 10336 samples\n","Epoch 1/3\n","41341/41341 [==============================] - 122s 3ms/step - loss: 0.6192 - acc: 0.6502 - val_loss: 0.5685 - val_acc: 0.7064\n","Epoch 2/3\n","41341/41341 [==============================] - 113s 3ms/step - loss: 0.5686 - acc: 0.7054 - val_loss: 0.5599 - val_acc: 0.7112\n","Epoch 3/3\n","41341/41341 [==============================] - 114s 3ms/step - loss: 0.5559 - acc: 0.7129 - val_loss: 0.5515 - val_acc: 0.7194\n","LSTM Model\n","Test Accuracy:0.5430738667148275\n","Test F1:0.35069934556653404\n","CNN + LSTM Model\n","Test Accuracy:0.6010926494491602\n","Test F1:0.4679954236165472\n","CNN + Bidirectional LSTM Model\n","Test Accuracy:0.5894437420986094\n","Test F1:0.3414210183240386\n","CNN + Bidirectional LSTM with GLOVE Model\n","Test Accuracy:[0.6485450126557697, 0.6250225754018421]\n"],"name":"stdout"}]}]}